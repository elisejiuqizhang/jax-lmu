{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legendre Memory Unit (LMU) for the permuted MNIST task\n",
    "Implementation based on:\n",
    "* the original NIPS 2019 paper on LMU (https://proceedings.neurips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html);\n",
    "* and the GitHub repository of the LMU PyTorch implementation (https://github.com/hrshtv/pytorch-lmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.numpy import fft\n",
    "from scipy.signal import cont2discrete\n",
    "\n",
    "from jax import random\n",
    "from jax.nn import initializers\n",
    "\n",
    "import flax.linen as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Cell of LMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMUCell(nn.Module):\n",
    "    \"\"\" \n",
    "    LMU Cell\n",
    "\n",
    "    Parameters:\n",
    "        input_size (int) : \n",
    "            Size of the input vector (x_t)\n",
    "        hidden_size (int) : \n",
    "            Size of the hidden vector (h_t)\n",
    "        memory_size (int) :\n",
    "            Size of the memory vector (m_t)\n",
    "        theta (int) :\n",
    "            The number of timesteps in the sliding window that is represented using the LTI system\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, memory_size, theta):\n",
    "        super(LMUCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "        self.f=nn.tanh() # activation function\n",
    "\n",
    "        # State space matrices: A, B\n",
    "        A, B = self.stateSpaceMatrices(memory_size, theta)\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "\n",
    "        # The Model parameters:\n",
    "        ## Encoding vectors\n",
    "        self.e_x=jnp.empty(1,input_size)\n",
    "        self.e_h=jnp.empty(1,hidden_size)\n",
    "        self.e_m=jnp.empty(1,memory_size)\n",
    "        ## Kernels\n",
    "        self.W_x=jnp.empty(hidden_size,input_size)\n",
    "        self.W_h=jnp.empty(hidden_size,hidden_size)\n",
    "        self.W_m=jnp.empty(hidden_size,memory_size)\n",
    "        ## Initialize parameters\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def stateSpaceMatrices(self, memory_size, theta):\n",
    "        \"\"\" Returns the discretized state space matrices A and B \"\"\"\n",
    "        Q = jnp.arange(memory_size, dtype = jnp.float64).reshape(-1, 1)\n",
    "        R = (2*Q + 1) / theta\n",
    "        i, j = jnp.meshgrid(Q, Q, indexing = \"ij\")\n",
    "\n",
    "        # Continuous\n",
    "        A = R * jnp.where(i < j, -1, (-1.0)**(i - j + 1))\n",
    "        B = R * ((-1.0)**Q)\n",
    "        C = jnp.ones((1, memory_size))\n",
    "        D = jnp.zeros((1,))\n",
    "\n",
    "        # Convert to discrete\n",
    "        A, B, C, D, dt = cont2discrete(\n",
    "                                system = (A, B, C, D), \n",
    "                                dt = 1.0, \n",
    "                                method = \"zoh\"\n",
    "                            )\n",
    "            \n",
    "        return A, B\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\" Initialize model parameters \"\"\"\n",
    "        self.e_x = initializers.lecun_uniform()(self.e_x)\n",
    "        self.e_h = initializers.lecun_uniform()(self.e_h)\n",
    "        self.e_m = initializers.constant(0.0)(self.e_m)\n",
    "        self.W_x = initializers.xavier_normal()(self.W_x)\n",
    "        self.W_h = initializers.xavier_normal()(self.W_h)\n",
    "        self.W_m = initializers.xavier_normal()(self.W_m)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x (array) : \n",
    "                Input vector of size [batch_size, input_size]\n",
    "            state (array) : \n",
    "                The hidden state:\n",
    "                h: [batch_size, hidden_size]\n",
    "                m: [batch_size, memory_size]\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the hidden state and memory\n",
    "        h, m = state\n",
    "\n",
    "        # Equation (7) of the paper\n",
    "        ## u: [batch_size, 1]\n",
    "        u=jnp.matmul(self.e_x,x)+jnp.matmul(self.e_h,h)+jnp.matmul(self.e_m,m)\n",
    "\n",
    "        # Equation (4) of the paper\n",
    "        ## m: [batch_size, memory_size]\n",
    "        m=jnp.matmul(self.A,m)+jnp.matmul(self.B,u)\n",
    "\n",
    "        # Equation (6) of the paper\n",
    "        ## h: [batch_size, hidden_size]\n",
    "        h=self.f(\n",
    "                jnp.matmul(self.W_x,x)\n",
    "                +jnp.matmul(self.W_h,h)\n",
    "                +jnp.matmul(self.W_m,m)\n",
    "            )\n",
    "\n",
    "        return h, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LMU Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMU(nn.Module):\n",
    "    \"\"\" \n",
    "    The LMU Layer\n",
    "    \n",
    "    Parameters:\n",
    "        input_size (int) : \n",
    "            Size of the input vector (x_t)\n",
    "        hidden_size (int) : \n",
    "            Size of the hidden vector (h_t)\n",
    "        memory_size (int) :\n",
    "            Size of the memory vector (m_t)\n",
    "        theta (int) :\n",
    "            The number of timesteps in the sliding window that is represented using the LTI system\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, memory_size, theta):\n",
    "        super(LMU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "        self.theta = theta\n",
    "\n",
    "        # LMU Cell\n",
    "        self.cell = LMUCell(input_size, hidden_size, memory_size, theta)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x (array):\n",
    "                Input vector of size [batch_size, seq_len, input_size]\n",
    "            state (array):\n",
    "                h: [batch_size, hidden_size]\n",
    "                m: [batch_size, memory_size]\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Assume the order of dimensions is [batch_size, seq_len, ......]\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # Initial State (h_0, m_0)\n",
    "        if state is None:\n",
    "            h_0 = jnp.zeros((batch_size, self.hidden_size))\n",
    "            m_0 = jnp.zeros((batch_size, self.memory_size))\n",
    "            if x.is_cuda: # If the input is on the GPU, put to device\n",
    "                h_0 = h_0.cuda()\n",
    "                m_0 = m_0.cuda()\n",
    "            state = (h_0, m_0)\n",
    "\n",
    "        # Iterate over time steps\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t=x[:,t,:] # x_t: [batch_size, input_size]\n",
    "            h_t, m_t = self.cell(x_t, state) # h_t: [batch_size, hidden_size], m_t: [batch_size, memory_size]\n",
    "            state=(h_t, m_t)\n",
    "            output.append(h_t)\n",
    "\n",
    "        output = jnp.stack(output) # output: [seq_len, batch_size, hidden_size]\n",
    "        output = output.transpose(1, 0, 2) # output: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data\n",
    "Load the MNIST data from PyTroch's DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The permuted MNIST dataset.\n",
    "The permutation matrix \"permutation.pt\" is from the LMU PyTorch implementation repo (https://github.com/hrshtv/pytorch-lmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class psMNIST(Dataset):\n",
    "    \"\"\" Dataset that defines the psMNIST dataset, given the MNIST data and a fixed permutation \"\"\"\n",
    "\n",
    "    def __init__(self, mnist, perm):\n",
    "        self.mnist = mnist # also a torch.data.Dataset object\n",
    "        self.perm  = perm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.mnist[idx]\n",
    "        unrolled = img.reshape(-1)\n",
    "        permuted = unrolled[self.perm]\n",
    "        permuted = permuted.reshape(-1, 1)\n",
    "        return permuted, label"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
