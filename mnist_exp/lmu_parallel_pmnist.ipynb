{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelized Legendre Memory Unit (LMU) on the permuted MNIST dataset\n",
    "Implementation based on \n",
    "* the ICML 2021 paper \"Parallelizing Legendre Memory Unit Training\" (https://proceedings.mlr.press/v139/chilkuri21a.html)\n",
    "* and the GitHub repository of the LMU PyTorch implementation (https://github.com/hrshtv/pytorch-lmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.numpy import fft\n",
    "from scipy.signal import cont2discrete\n",
    "\n",
    "from jax import random\n",
    "from jax.nn import initializers\n",
    "\n",
    "import flax.linen as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "The LMUFFT Layer Based on the 2021 ICML paper https://proceedings.mlr.press/v139/chilkuri21a.html.\n",
    "Specifically following Equation (26) in the paper which uses fast Fourier transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMUFFT(nn.Module):\n",
    "    \"\"\"\n",
    "    Parallelized LMU Layer\n",
    "\n",
    "    Parameters:\n",
    "        input_size (int) : \n",
    "            Size of the input vector (x_t)\n",
    "        hidden_size (int) : \n",
    "            Size of the hidden vector (h_t)\n",
    "        memory_size (int) :\n",
    "            Size of the memory vector (m_t)\n",
    "        seq_len (int) :\n",
    "            Size of the sequence length (n)\n",
    "        theta (int) :\n",
    "            The number of timesteps in the sliding window that is represented using the LTI system\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, memory_size, seq_len, theta):\n",
    "        super(LMUFFT, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "        self.seq_len = seq_len\n",
    "        self.theta = theta\n",
    "\n",
    "        self.W_u=nn.linear(input_size, 1) # in_features = input_size, out_features = 1\n",
    "        self.f_u=nn.relu()\n",
    "        self.W_h=nn.linear(memory_size + input_size, hidden_size) # in_features = memory_size + input_size, out_features = hidden_size\n",
    "        self.f_h=nn.relu()\n",
    "\n",
    "        # A: # [memory_size, memory_size]\n",
    "        # B: # [memory_size, 1]        \n",
    "        A, B = self.stateSpaceMatrices()\n",
    "\n",
    "        # H: [memory_size, seq_len]\n",
    "        # fft_H: [memory_size, seq_len + 1]\n",
    "        H, fft_H = self.impulse()\n",
    "\n",
    "\n",
    "\n",
    "    def stateSpaceMatrices(self):\n",
    "        \"\"\" Returns the discretized state space matrices A and B \"\"\"\n",
    "        Q = jnp.arange(self.memory_size, dtype = jnp.float64).reshape(-1, 1)\n",
    "        R = (2*Q + 1) / self.theta\n",
    "        i, j = jnp.meshgrid(Q, Q, indexing = \"ij\")\n",
    "\n",
    "        # Continuous\n",
    "        A = R * jnp.where(i < j, -1, (-1.0)**(i - j + 1))\n",
    "        B = R * ((-1.0)**Q)\n",
    "        C = jnp.ones((1, self.memory_size))\n",
    "        D = jnp.zeros((1,))\n",
    "\n",
    "        # Convert to discrete\n",
    "        A, B, C, D, dt = cont2discrete(\n",
    "                                system = (A, B, C, D), \n",
    "                                dt = 1.0, \n",
    "                                method = \"zoh\"\n",
    "                                )\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    def impulse(self):\n",
    "        \"\"\" Returns the matrices H and the 1D Fourier transform of H (Equations 23, 26 of the paper) \"\"\"\n",
    "            \n",
    "        H = []\n",
    "        A_i=jnp.eye(self.memory_size, dtype = jnp.float64)\n",
    "        for i in range(self.seq_len):\n",
    "            H.append(jnp.matmul(A_i, self.B))\n",
    "            A_i = jnp.matmul(self.A, A_i)\n",
    "\n",
    "        H=jnp.concatenate(H, axis = -1) # H: [memory_size, seq_len]\n",
    "        fft_H=fft.rfft(H, n = 2*self.seq_len, dim = -1) # [memory_size, seq_len + 1]\n",
    "\n",
    "        return H, fft_H\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x (array):\n",
    "                Input of size [batch_size, seq_len, input_size]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, input_size = x.shape\n",
    "\n",
    "        # Equation 18 of the paper\n",
    "        u=self.f_u(self.W_u(x)) # u: [batch_size, seq_len, 1]\n",
    "\n",
    "        # Equation 26 of the paper\n",
    "        fft_input=u.transpose(0, 2, 1) # fft_input: [batch_size, 1, seq_len]\n",
    "        fft_u=fft.rfft(fft_input, n = 2*self.seq_len, dim = -1) # fft_u: [batch_size, 1, seq_len + 1]\n",
    "\n",
    "        # Element-wise multiplication (under broadcasting)\n",
    "        # [batch_size, 1, seq_len+1] * [1, memory_size, seq_len+1]\n",
    "        temp=fft_u * self.fft_H.expand_dims(0) # temp: [batch_size, memory_size, seq_len+1]\n",
    "\n",
    "        m = fft.irfft(temp, n = 2*seq_len, dim = -1) # m: [batch_size, memory_size, seq_len+1]\n",
    "        m = m[:, :, :seq_len] # m: [batch_size, memory_size, seq_len]\n",
    "        m = m.transpose(0, 2, 1) # m: [batch_size, seq_len, memory_size]\n",
    "\n",
    "        # Equation 20 of the paper \n",
    "        input_h=jnp.concatenate([m,x], axis = -1) # input_h: [batch_size, seq_len, memory_size + input_size]])\n",
    "        h=self.f_h(self.W_h(input_h)) # h: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        h_n=h[:, -1, :] # h_n: [batch_size, hidden_size]\n",
    "        \n",
    "        return h, h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data\n",
    "Load the MNIST data from PyTroch's DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The permuted MNIST dataset.\n",
    "The permutation matrix \"permutation.pt\" is from the LMU PyTorch implementation repo (https://github.com/hrshtv/pytorch-lmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class psMNIST(Dataset):\n",
    "    \"\"\" Dataset that defines the psMNIST dataset, given the MNIST data and a fixed permutation \"\"\"\n",
    "\n",
    "    def __init__(self, mnist, perm):\n",
    "        self.mnist = mnist # also a torch.data.Dataset object\n",
    "        self.perm  = perm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.mnist[idx]\n",
    "        unrolled = img.reshape(-1)\n",
    "        permuted = unrolled[self.perm]\n",
    "        permuted = permuted.reshape(-1, 1)\n",
    "        return permuted, label"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
