{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legendre Memory Units (LMUs) for MNIST classification\n",
    "Implementation based on:\n",
    "* the original NIPS 2019 paper on LMU (https://proceedings.neurips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html);\n",
    "* and the GitHub repository of the LMU PyTorch implementation (https://github.com/hrshtv/pytorch-lmu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-26 18:14:56.906933: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-26 18:14:56.907014: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-26 18:14:56.907021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/usr/local/data/elisejzh/anaconda3/envs/jax-lmu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, Mapping, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "\n",
    "from flax import linen as nn\n",
    "\n",
    "from scipy.signal import cont2discrete\n",
    "\n",
    "import tensorflow_datasets as tfds \n",
    "\n",
    "import optax\n",
    "from flax.training import train_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------- Cell Base ----------------------#\n",
    "# The other classes inherit from this class\n",
    "\n",
    "class RecurrentCellBase(nn.Module):\n",
    "  \"\"\"Recurrent cell base class.\"\"\"\n",
    "\n",
    "  @staticmethod\n",
    "  def initialize_carry(batch_dims, size, init_fn=nn.initializers.zeros):\n",
    "    \"\"\"Initialize the RNN cell carry.\n",
    "\n",
    "    Args:\n",
    "      batch_dims: a tuple providing the shape of the batch dimensions.\n",
    "      size: the size or number of features of the memory.\n",
    "      init_fn: initializer function for the carry.\n",
    "    Returns:\n",
    "      An initialized carry for the given cell.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------- One LMU Cell ----------------------#\n",
    "# References: \n",
    "# https://flax.readthedocs.io/en/latest/_modules/flax/linen/recurrent.html\n",
    "# https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.scan.html\n",
    "\n",
    "class LMUCell(RecurrentCellBase):\n",
    "    \"\"\"LMU Cell\n",
    "    \n",
    "    Parameters:\n",
    "        input_size (int) : \n",
    "            Size/dimensions of the input vector (x_t)\n",
    "        hidden_size (int) : \n",
    "            Size of the hidden vector (h_t)\n",
    "        memory_size (int) :\n",
    "            Size of the memory vector (m_t)\n",
    "        theta (int) :\n",
    "            The number of timesteps in the sliding window that is represented using the LTI system  \n",
    "    \n",
    "    Other Attributes:\n",
    "        State Space Matrices:\n",
    "            A: [memory_size, memory_size]\n",
    "            B: [memory_size, 1]\n",
    "        Initializers:\n",
    "            init_lecun_uni: lecun_normal, for two encoding vectors, e_x and e_h\n",
    "            init_zero: constant(0), for the encoding vector e_m\n",
    "            init_xav_norm: xavier_normal, for three kernel matrices, W_x, W_h, W_m\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    memory_size: int\n",
    "    theta: int\n",
    "\n",
    "    init_lecun_uni: Callable = nn.initializers.lecun_normal()\n",
    "    init_zero: Callable = nn.initializers.constant(0.)\n",
    "    init_xav_norm: Callable = nn.initializers.xavier_normal()\n",
    "\n",
    "    activation_fn: Callable = nn.tanh\n",
    "    # activation_fn: Callable = nn.relu\n",
    "\n",
    "    def setup(self):\n",
    "        self.A, self.B = self.stateSpaceMatrices()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            carry (tuple): the previous state:\n",
    "                h: the hidden vector, [batch_size, hidden_size]\n",
    "                m: the memory vector, [batch_size, memory_size]\n",
    "                \n",
    "            x (array): the input vector,[batch_size, input_size]\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the hidden state and memory\n",
    "        h, m = carry\n",
    "\n",
    "        # Eq (7) of the paper, compute the input signal u\n",
    "        # u: [batch_size, 1]\n",
    "        u_x = nn.Dense(features=1,\n",
    "                        use_bias=False,\n",
    "                        kernel_init=self.init_lecun_uni,\n",
    "                        bias_init=self.init_zero,\n",
    "                        param_dtype=jnp.float32)\n",
    "        u_h = nn.Dense(features=1,\n",
    "                        use_bias=False,\n",
    "                        kernel_init=self.init_lecun_uni,\n",
    "                        bias_init=self.init_zero,\n",
    "                        param_dtype=jnp.float32)\n",
    "        u_m = nn.Dense(features=1,\n",
    "                        use_bias=False,\n",
    "                        kernel_init=self.init_zero,\n",
    "                        bias_init=self.init_zero,\n",
    "                        param_dtype=jnp.float32)\n",
    "        u = u_x(x) + u_h(h) + u_m(m)\n",
    "\n",
    "\n",
    "        # Eq (4) of the paper, compute the memory\n",
    "        # m: [batch_size, memory_size]\n",
    "        m_m = jnp.matmul(m, self.A) \n",
    "        m_u = jnp.matmul(u, self.B.T)\n",
    "        new_m= m_m + m_u\n",
    "\n",
    "        # Eq (6) of the paper, compute the hidden state\n",
    "        # h: [batch_size, hidden_size]\n",
    "        h_x = nn.Dense(features=self.hidden_size,\n",
    "                        use_bias=False,\n",
    "                        kernel_init=self.init_xav_norm,\n",
    "                        bias_init=self.init_zero,\n",
    "                        param_dtype=jnp.float32)\n",
    "        h_h = nn.Dense(features=self.hidden_size,\n",
    "                        use_bias=False,\n",
    "                        kernel_init=self.init_xav_norm,\n",
    "                        bias_init=self.init_zero,\n",
    "                        param_dtype=jnp.float32)\n",
    "        h_m = nn.Dense(features=self.hidden_size,\n",
    "                        use_bias=False,\n",
    "                        kernel_init=self.init_xav_norm,\n",
    "                        bias_init=self.init_zero,\n",
    "                        param_dtype=jnp.float32)\n",
    "        new_h = self.activation_fn(h_x(x) + h_h(h) + h_m(m))\n",
    "        return (new_h, new_m), new_h\n",
    "\n",
    "    @staticmethod\n",
    "    def initialized_carry(batch_size, hidden_size, memory_size):\n",
    "        \"\"\"Initialize the LMU cell carry.\n",
    "        Args:\n",
    "            batch_size (int) : \n",
    "                Size of the batch\n",
    "            hidden_size (int) : \n",
    "                Size of the hidden vector (h_t)\n",
    "            memory_size (int) :\n",
    "                Size of the memory vector (m_t)\n",
    "        Returns:\n",
    "            An initialized carry for the given cell.\n",
    "        \"\"\"\n",
    "        h = jnp.zeros((batch_size, hidden_size))\n",
    "        m = jnp.zeros((batch_size, memory_size))\n",
    "        return h, m\n",
    "\n",
    "    \n",
    "\n",
    "    def stateSpaceMatrices(self):\n",
    "        \"\"\" Returns the discretized state space matrices A and B \"\"\"\n",
    "        Q = np.arange(self.memory_size, dtype = np.float32)\n",
    "        R = (2*Q + 1) / self.theta\n",
    "        i, j = np.meshgrid(Q, Q, indexing = \"ij\")\n",
    "\n",
    "        # Continuous\n",
    "        A = R * np.where(i < j, -1, (-1.0)**(i - j + 1))\n",
    "        B = R * ((-1.0)**Q)\n",
    "        B = B.reshape(-1, 1)\n",
    "        C = np.ones((1, self.memory_size))\n",
    "        D = np.zeros((1,))\n",
    "\n",
    "        # Convert to discrete\n",
    "        A, B, C, D, dt = cont2discrete(\n",
    "                                system = (A, B, C, D), \n",
    "                                dt = 1.0, \n",
    "                                method = \"zoh\"\n",
    "                            )\n",
    "            \n",
    "        return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------- LMU Layer ----------------------#\n",
    "# Reference: https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.scan.html\n",
    "\n",
    "class LMU(nn.Module):\n",
    "    \"\"\"\n",
    "    The LMU Layer\n",
    "\n",
    "    Parameters:\n",
    "        input_size (int) : \n",
    "            Size of the input vector (x_t)\n",
    "        hidden_size (int) : \n",
    "            Size of the hidden vector (h_t)\n",
    "        memory_size (int) :\n",
    "            Size of the memory vector (m_t)\n",
    "        theta (int) :\n",
    "            The number of timesteps in the sliding window that is represented using the LTI system\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    memory_size: int\n",
    "    theta: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            carry (tuple): the previous state, set to None initially\n",
    "                h: the hidden vector, [batch_size, hidden_size]\n",
    "                m: the memory vector, [batch_size, memory_size]\n",
    "            x (array): the input vector, [batch_size, seq_len, input_size]\n",
    "        \"\"\"\n",
    "\n",
    "        LMULayer = nn.scan(LMUCell,\n",
    "                            variable_broadcast=\"params\",\n",
    "                            split_rngs={\"params\": False},\n",
    "                            in_axes=1,\n",
    "                            out_axes=1)\n",
    "\n",
    "        return LMULayer(self.input_size, self.hidden_size, self.memory_size, self.theta)(carry, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_carry(batch_size, hidden_size, memory_size):\n",
    "        return LMUCell.initialized_carry(batch_size, hidden_size, memory_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- LMU Model for pMNIST Classification ----------------#\n",
    "class Model(nn.Module):\n",
    "    input_size: int\n",
    "    output_size: int\n",
    "    hidden_size: int\n",
    "    memory_size: int\n",
    "    theta: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # Initialize the carry at the first time step\n",
    "        # Get initial carry/state (h_0, m_0)\n",
    "        init_carry = LMU.initialize_carry(x.shape[0], self.hidden_size, self.memory_size)\n",
    "\n",
    "        # Run the LMU layer\n",
    "        # h_n: [batch_size, hidden_size]\n",
    "        (h_n , _),_= LMU(self.input_size, self.hidden_size, self.memory_size, self.theta)(init_carry, x)\n",
    "\n",
    "        # Run the output layer\n",
    "        output=nn.Dense(features=self.output_size)(h_n)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_x = 1 # dimension of the input, a single pixel\n",
    "N_t = 784 # number of time steps (sequence length) - here it's 28 * 28 since we are using MNIST and making it 1D\n",
    "N_h = 256 # dimension of the hidden state\n",
    "N_m = 256 # dimension of the memory\n",
    "N_c = 10 # number of classes \n",
    "THETA = N_t\n",
    "N_b = 512 # batch size\n",
    "N_epochs = 10 # number of epochs\n",
    "\n",
    "\n",
    "lr=1e-4 # learning rate for adam optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-26 18:14:57.968049: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-26 18:14:57.968070: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def get_datasets():\n",
    "    \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "\n",
    "    ds_builder = tfds.builder('mnist')\n",
    "    ds_builder.download_and_prepare()\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    val_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "\n",
    "\n",
    "    train_ds['image'] = jnp.float32(train_ds['image'])\n",
    "    val_ds['image'] = jnp.float32(val_ds['image'])\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "\n",
    "train_ds, val_ds = get_datasets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Following the Flax example: https://flax.readthedocs.io/en/latest/getting_started.html\"\"\"\n",
    "\n",
    "def cross_entropy_loss(*, logits, labels):\n",
    "    labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean() \n",
    "\n",
    "def compute_metrics(*, logits, labels):\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate=lr):\n",
    "    model = Model(\n",
    "            input_size  = N_x,\n",
    "            output_size = N_c,\n",
    "            hidden_size = N_h, \n",
    "            memory_size = N_m, \n",
    "            theta = THETA\n",
    "        )\n",
    "\n",
    "    \n",
    "    params = model.init(rng, jnp.ones((1, N_t, N_x)))['params']\n",
    "    print(\"Model initialized.\")\n",
    "\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits=logits, labels=batch['label'])\n",
    "        return loss\n",
    "    grad_fn = value_and_grad(loss_fn, has_aux=False)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    logits = new_state.apply_fn({'params': new_state.params}, batch['image'])\n",
    "    metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "\n",
    "    return new_state, metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(state, batch, seq_len=N_t, input_size=N_x):\n",
    "    batch['image']=batch['image'].reshape((-1, seq_len, input_size))\n",
    "    logits = state.apply_fn({'params': state.params}, batch['image'])\n",
    "    return compute_metrics(logits=logits, labels=batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, epoch, rng, batch_size=N_b, seq_len=N_x, input_size=N_x):\n",
    "\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size) # get a randomized index array\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size)) # index array, where each row is a batch\n",
    "\n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()} # dict{'image': array, 'label': array}\n",
    "        batch['image']=batch['image'].reshape((batch_size, -1, input_size)) # reshape to the required input dimensions\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "    \n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]} # jnp.mean does not work on lists\n",
    "\n",
    "    print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(state, test_ds):\n",
    "    metrics = eval_step(state, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_util.tree_map(lambda x: x.item(), metrics) # map the function over all leaves in metrics\n",
    "    return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n",
      "train epoch: 0, loss: 1.1877, accuracy: 68.73\n",
      " test epoch: 0, loss: 0.62, accuracy: 86.53\n",
      "train epoch: 1, loss: 0.4881, accuracy: 88.41\n",
      " test epoch: 1, loss: 0.38, accuracy: 90.42\n",
      "train epoch: 2, loss: 0.3562, accuracy: 90.78\n",
      " test epoch: 2, loss: 0.30, accuracy: 91.88\n",
      "train epoch: 3, loss: 0.2892, accuracy: 92.30\n",
      " test epoch: 3, loss: 0.26, accuracy: 92.98\n"
     ]
    }
   ],
   "source": [
    "# Random seed\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "# Initialize model\n",
    "state=create_train_state(rng, learning_rate=lr)\n",
    "del init_rng\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    # Use a separate PRNG key to permute image data during shuffling\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state = train_epoch(state, train_ds, epoch, input_rng)\n",
    "    test_loss, test_accuracy = eval_model(state, val_ds)\n",
    "    print(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-lmu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "affe042c54b706202da5ddcf554e5ef1417f77dada3b849bebf4673262c6ea4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
