{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelized Legendre Memory Unit (LMU) on the permuted MNIST dataset\n",
    "Implementation based on \n",
    "* the ICML 2021 paper \"Parallelizing Legendre Memory Unit Training\" (https://proceedings.mlr.press/v139/chilkuri21a.html)\n",
    "* and the GitHub repository of the LMU PyTorch implementation (https://github.com/hrshtv/pytorch-lmu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 14:59:13.918236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-05 14:59:13.918278: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-05 14:59:13.918282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/usr/local/data/elisejzh/anaconda3/envs/jax-lmu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, Mapping, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "\n",
    "from flax import linen as nn\n",
    "\n",
    "from scipy.signal import cont2discrete\n",
    "\n",
    "import tensorflow_datasets as tfds \n",
    "\n",
    "import optax\n",
    "from flax.training import train_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------- Parallelized LMU ----------------------#\n",
    "class LMUFFT(nn.Module):\n",
    "    \"\"\" Parallelized LMU Layer\n",
    "        \n",
    "        Parameters:\n",
    "            input_size (int) : \n",
    "                Size of the input vector (x_t)\n",
    "            hidden_size (int) : \n",
    "                Size of the hidden vector (h_t)\n",
    "            memory_size (int) :\n",
    "                Size of the memory vector (m_t)\n",
    "            seq_len (int) :\n",
    "                Size of the sequence length (n)\n",
    "            theta (int) :\n",
    "                The number of timesteps in the sliding window that is represented using the LTI system\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    memory_size: int\n",
    "    seq_len: int\n",
    "    theta: int\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        A: [memory_size, memory_size]\n",
    "        B: [memory_size, 1]\n",
    "        \"\"\"\n",
    "        self.A, self.B = self.stateSpaceMatrices() # numpy\n",
    "        self.H, self.H_fft = self.impulse() # numpy\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            x: [batch_size, seq_len, input_size];\n",
    "\n",
    "        Returns:\n",
    "            h: [batch_size, seq_len, hidden_size]; The parallelized/flattened hidden states of every timestep;\n",
    "            h_n: [batch_size, hidden_size]; The hidden state of the last timestep;\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_size = x.shape\n",
    "\n",
    "        # Equation 18 of the paper\n",
    "        u_pre_act = nn.Dense(features=1,\n",
    "                            use_bias=True)\n",
    "        u = nn.relu(u_pre_act(x)) # [batch_size, seq_len, 1]\n",
    "\n",
    "        # Equation 26 of the paper\n",
    "        fft_input=u.transpose(0, 2, 1) # [batch_size, 1, seq_len]\n",
    "        fft_u=jnp.fft.rfft(fft_input, n = 2*seq_len, axis = -1) # [batch_size, seq_len, seq_len+1]\n",
    "\n",
    "        # Element-wise multiplication (uses broadcasting)\n",
    "        # fft_u:[batch_size, 1, seq_len+1] \n",
    "        # self.H_fft: [memory_size, seq_len+1] -> to be expanded in dimension 0\n",
    "        H_fft=self.H_fft.reshape(1, self.memory_size, self.seq_len+1) # [1, memory_size, seq_len+1]\n",
    "        # [batch_size, 1, seq_len+1] * [1, memory_size, seq_len+1]\n",
    "        temp=jnp.multiply(fft_u, H_fft) # [batch_size, memory_size, seq_len+1]\n",
    "\n",
    "        m=jnp.fft.irfft(temp, n = 2*seq_len, axis = -1) # [batch_size, memory_size, seq_len+1]\n",
    "        m=m[:, :, :seq_len] # [batch_size, memory_size, seq_len]\n",
    "        m=m.transpose(0, 2, 1) # [batch_size, seq_len, memory_size]\n",
    "\n",
    "        # Equation 20 of the paper\n",
    "        input_h=jnp.concatenate((x, m), axis=-1) # [batch_size, seq_len, input_size + memory_size]\n",
    "        h_pre_act = nn.Dense(features=self.hidden_size,\n",
    "                            use_bias=True)\n",
    "\n",
    "        # h=nn.tanh(h_pre_act(input_h)) # [batch_size, seq_len, hidden_size]\n",
    "        h=nn.relu(h_pre_act(input_h)) # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        h_n=h[:, -1, :] # [batch_size, hidden_size]\n",
    "\n",
    "        return h, h_n\n",
    "\n",
    "\n",
    "\n",
    "    # def impulse(self):\n",
    "    #     def impulse_body(n, carry:Tuple):\n",
    "    #         \"\"\"The body function to be used in the fori_loop of the impluse function above\"\"\"\n",
    "    #         H, A_i=carry\n",
    "    #         H_n=jnp.matmul(A_i, self.B) # [memory_size, 1]\n",
    "    #         # H.append(H_n)\n",
    "    #         H = H.at[:, n].set(H_n.reshape(-1))\n",
    "    #         A_i=jnp.matmul(self.A, A_i)\n",
    "    #         return (H, A_i)\n",
    "    #     \"\"\" Returns the matrices H and the 1D Fourier transform of H (Equations 23, 26 of the paper) \"\"\"\n",
    "    #     H_init=jnp.empty((self.memory_size, self.seq_len)) # [memory_size, seq_len]\n",
    "    #     A_i_init=jnp.eye(self.memory_size, dtype = np.float32) # [memory_size, memory_size]\n",
    "    #     val_init = (H_init, A_i_init)\n",
    "    #     H, A_i=lax.fori_loop(0, self.seq_len, impulse_body, val_init)        \n",
    "    #     # H=np.concatenate(H_fin, axis=-1) # [memory_size, seq_len]\n",
    "    #     H_fft=np.fft.rfft(H, n = 2*self.seq_len, axis = -1) # [memory_size, seq_len + 1]\n",
    "    #     return H, H_fft\n",
    "\n",
    "    def impulse(self):\n",
    "        \"\"\" Returns the matrices H and the 1D Fourier transform of H (Equations 23, 26 of the paper) \"\"\"\n",
    "        H = np.empty((self.memory_size, self.seq_len), dtype = np.float32) # [memory_size, seq_len]\n",
    "        A_i = np.eye(self.memory_size, dtype = np.float32) # [memory_size, memory_size]\n",
    "        for n in range(self.seq_len):\n",
    "            H_n = np.matmul(A_i, self.B) # [memory_size, 1]\n",
    "            H[:, n]=H_n.reshape(-1)\n",
    "            A_i = np.matmul(self.A, A_i)\n",
    "        H_fft = np.fft.rfft(H, n = 2*self.seq_len, axis = -1) # [memory_size, seq_len + 1]\n",
    "        return H, H_fft\n",
    "\n",
    "    def stateSpaceMatrices(self):\n",
    "        \"\"\" Returns the discretized state space matrices A and B \"\"\"\n",
    "        Q = np.arange(self.memory_size, dtype = np.float32)\n",
    "        R = (2*Q + 1) / self.theta\n",
    "        i, j = np.meshgrid(Q, Q, indexing = \"ij\")\n",
    "\n",
    "        # Continuous\n",
    "        A = R * np.where(i < j, -1, (-1.0)**(i - j + 1))\n",
    "        B = R * ((-1.0)**Q)\n",
    "        B = B.reshape(-1, 1)\n",
    "        C = np.ones((1, self.memory_size))\n",
    "        D = np.zeros((1,))\n",
    "\n",
    "        # Convert to discrete\n",
    "        A, B, C, D, dt = cont2discrete(\n",
    "                                system = (A, B, C, D), \n",
    "                                dt = 1.0, \n",
    "                                method = \"zoh\"\n",
    "                            )\n",
    "            \n",
    "        return A, B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------- Parallelized LMU for pMNIST Classification ----------------------#\n",
    "class Model(nn.Module):\n",
    "    input_size: int\n",
    "    output_size: int\n",
    "    hidden_size: int\n",
    "    memory_size: int\n",
    "    theta: int\n",
    "    seq_len:int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        _, h_n = LMUFFT(input_size=self.input_size, hidden_size=self.hidden_size, memory_size=self.memory_size, seq_len=self.seq_len, theta=self.theta)(x)\n",
    "        x = nn.relu(h_n)\n",
    "        output = nn.Dense(features=self.output_size)(x)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_x = 1 # dimension of the input, a single pixel\n",
    "N_t = 784 # number of time steps (sequence length) - here it's 28 * 28 since we are using MNIST and making it 1D\n",
    "N_h = 128 # dimension of the hidden state\n",
    "N_m = 64 # dimension of the memory\n",
    "N_c = 10 # number of classes \n",
    "THETA = N_t\n",
    "N_b = 200 # batch size\n",
    "N_epochs = 20 # number of epochs\n",
    "\n",
    "\n",
    "lr=1e-4 # learning rate for adam optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 14:59:14.715023: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-01-05 14:59:14.715047: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def get_datasets():\n",
    "    \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "\n",
    "    ds_builder = tfds.builder('mnist')\n",
    "    ds_builder.download_and_prepare()\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    val_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "\n",
    "\n",
    "    train_ds['image'] = jnp.float32(train_ds['image'])\n",
    "    val_ds['image'] = jnp.float32(val_ds['image'])\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "train_ds, val_ds = get_datasets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Following the Flax example: https://flax.readthedocs.io/en/latest/getting_started.html\"\"\"\n",
    "\n",
    "def cross_entropy_loss(*, logits, labels):\n",
    "    labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean() \n",
    "\n",
    "def compute_metrics(*, logits, labels):\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate=lr):\n",
    "    model = Model(\n",
    "            input_size  = N_x,\n",
    "            output_size = N_c,\n",
    "            hidden_size = N_h, \n",
    "            memory_size = N_m, \n",
    "            seq_len=N_t,\n",
    "            theta = THETA\n",
    "        )\n",
    "\n",
    "    \n",
    "    params = model.init(rng, jnp.ones((1, N_t, N_x)))['params']\n",
    "    print(\"Model initialized.\")\n",
    "\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits=logits, labels=batch['label'])\n",
    "        return loss\n",
    "    grad_fn = value_and_grad(loss_fn, has_aux=False)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    logits = new_state.apply_fn({'params': new_state.params}, batch['image'])\n",
    "    metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "\n",
    "    return new_state, metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(state, batch, seq_len=N_t, input_size=N_x):\n",
    "    batch['image']=batch['image'].reshape((-1, seq_len, input_size))\n",
    "    logits = state.apply_fn({'params': state.params}, batch['image'])\n",
    "    return compute_metrics(logits=logits, labels=batch['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, epoch, rng, batch_size=N_b, seq_len=N_x, input_size=N_x):\n",
    "\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size) # get a randomized index array\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size)) # index array, where each row is a batch\n",
    "\n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()} # dict{'image': array, 'label': array}\n",
    "        batch['image']=batch['image'].reshape((batch_size, -1, input_size)) # reshape to the required input dimensions\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "    \n",
    "    # compute mean of metrics across each batch in epoch.\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]} # jnp.mean does not work on lists\n",
    "\n",
    "    print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(state, test_ds):\n",
    "    metrics = eval_step(state, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_util.tree_map(lambda x: x.item(), metrics) # map the function over all leaves in metrics\n",
    "    return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "# Initialize model\n",
    "state=create_train_state(rng, learning_rate=lr)\n",
    "del init_rng\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    # Use a separate PRNG key to permute image data during shuffling\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state = train_epoch(state, train_ds, epoch, input_rng)\n",
    "    test_loss, test_accuracy = eval_model(state, val_ds)\n",
    "    print(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-lmu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "affe042c54b706202da5ddcf554e5ef1417f77dada3b849bebf4673262c6ea4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
